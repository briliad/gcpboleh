#GCP Boleh notes

GCP Boleh Notes

bq query --use_legacy_sql=false \
'SELECT
   word
 FROM
   `bigquery-public-data`.samples.shakespeare
 WHERE
   word = "huzzah"'


bq query --use_legacy_sql=false \
'SELECT
   * 
 FROM
   `bigquery-public-data`.oxford_policy_tracker
 WHERE
   (alpha_3_code!="GBR") OR (alpha_3_code!="BRA") OR( alpha_3_code!="CAN") OR (alpha_3_code!="USA")'


CREATE OR REPLACE TABLE covid_451.oxford_policy_tracker_832
 PARTITION BY date
 OPTIONS (
   partition_expiration_days=360,
   description="govt policy tracker"
 ) AS
 SELECT
   * 
 FROM
   `bigquery-public-data`.covid19_govt_response.oxford_policy_tracker
 WHERE
   (alpha_3_code!="GBR") AND (alpha_3_code!="BRA") AND ( alpha_3_code!="CAN") AND (alpha_3_code!="USA")

curl -LO http://www.ssa.gov/OACT/babynames/names.zip

bq load babynames.names2010 yob2010.txt name:string,gender:string,count:integer

bq ls babynames

bq show babynames.names2010

bq query "SELECT name,count FROM babynames.names2010 WHERE gender = 'F' ORDER BY count DESC LIMIT 5"

bq query "SELECT name,count FROM babynames.names2010 WHERE gender = 'M' ORDER BY count ASC LIMIT 5"



create or replace TABLE ecommerce.products AS 
SELECT
*
FROM
`data-to-insights.ecommerce.products`



# pull what sold on 08/01/2017
CREATE OR REPLACE TABLE ecommerce.sales_by_sku_20170801 AS
SELECT DISTINCT
  productSKU,
  SUM(IFNULL(productQuantity,0)) AS total_ordered
FROM
  `data-to-insights.ecommerce.all_sessions_raw`
WHERE date = '20170801'
GROUP BY productSKU
ORDER BY total_ordered DESC #462 skus sold


SELECT * FROM ecommerce.sales_by_sku_20170801
UNION ALL
SELECT * FROM ecommerce.sales_by_sku_20170802

Note: The difference between a UNION and UNION ALL is that a UNION will not include duplicate records.

SELECT * FROM `ecommerce.sales_by_sku_2017*`

SELECT * FROM `ecommerce.sales_by_sku_2017*`
WHERE _TABLE_SUFFIX = '0802'


Overview
BigQuery is Google's fully managed, NoOps, low cost analytics database. With BigQuery you can query terabytes and terabytes of data without having any infrastructure to manage or needing a database administrator. BigQuery uses SQL and can take advantage of the pay-as-you-go model. BigQuery allows you to focus on analyzing data to find meaningful insights.



Instead of scanning the entire dataset and filtering on a date field like we did in the earlier queries, we will now setup a date-partitioned table. This will allow us to completely ignore scanning records in certain partitions if they are irrelevant to our query.




#standardSQL
 CREATE OR REPLACE TABLE ecommerce.partition_by_day
 PARTITION BY date_formatted
 OPTIONS(
   description="a table partitioned by date"
 ) AS
 SELECT DISTINCT
 PARSE_DATE("%Y%m%d", date) AS date_formatted,
 fullvisitorId
 FROM `data-to-insights.ecommerce.all_sessions_raw`


 Creating an auto-expiring partitioned table
Auto-expiring partitioned tables are used to comply with data privacy statutes, and can be used to avoid unnecessary storage (which you'll be charged for in a production environment). If you want to create a rolling window of data, add an expiration date so the partition disappears after you're finished using it.


#standardSQL
 SELECT
   DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) AS date,
   (SELECT ANY_VALUE(name) FROM `bigquery-public-data.noaa_gsod.stations` AS stations
    WHERE stations.usaf = stn) AS station_name,  -- Stations may have multiple names
   prcp
 FROM `bigquery-public-data.noaa_gsod.gsod*` AS weather
 WHERE prcp < 99.9  -- Filter unknown values
   AND prcp > 0      -- Filter stations/days with no precipitation
   AND _TABLE_SUFFIX >= '2018'
 ORDER BY date DESC -- Where has it rained/snowed recently
 LIMIT 10


 -- with partition expiration
 CREATE OR REPLACE TABLE ecommerce.days_with_rain
 PARTITION BY date
 OPTIONS (
   partition_expiration_days=60,
   description="weather stations with precipitation, partitioned by day"
 ) AS
 SELECT
   DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) AS date,
   (SELECT ANY_VALUE(name) FROM `bigquery-public-data.noaa_gsod.stations` AS stations
    WHERE stations.usaf = stn) AS station_name,  -- Stations may have multiple names
   prcp
 FROM `bigquery-public-data.noaa_gsod.gsod*` AS weather
 WHERE prcp < 99.9  -- Filter unknown values
   AND prcp > 0      -- Filter stations/days with no precipitation
   AND _TABLE_SUFFIX >= '2018'


To confirm you are only storing data from 60 days in the past up until today, run the DATE_DIFF query to get the age of your partitions, which are set to expire after 60 days.


#standardSQL
# avg monthly precipitation
SELECT
  AVG(prcp) AS average,
  station_name,
  date,
  CURRENT_DATE() AS today,
  DATE_DIFF(CURRENT_DATE(), date, DAY) AS partition_age,
  EXTRACT(MONTH FROM date) AS month
FROM ecommerce.days_with_rain
WHERE station_name = 'WAKAYAMA' #Japan
GROUP BY station_name, date, today, month, partition_age
ORDER BY date DESC; # most recent days first



#String aggregate
Notice we use the STRING_AGG() function to aggregate all the product SKUs that are associated with one product name into comma separated values.

SELECT
  v2ProductName,
  COUNT(DISTINCT productSKU) AS SKU_count,
  STRING_AGG(DISTINCT productSKU LIMIT 5) AS SKU
FROM `data-to-insights.ecommerce.all_sessions_raw`
  WHERE productSKU IS NOT NULL
  GROUP BY v2ProductName
  HAVING SKU_count > 1
  ORDER BY SKU_count DESC   


  WITH inventory_per_sku AS (
  SELECT DISTINCT
    website.v2ProductName,
    website.productSKU,
    inventory.stockLevel
  FROM `data-to-insights.ecommerce.all_sessions_raw` AS website
  JOIN `data-to-insights.ecommerce.products` AS inventory
    ON website.productSKU = inventory.SKU
    WHERE productSKU = 'GGOEGPJC019099'
)
SELECT
  productSKU,
  SUM(stockLevel) AS total_inventory
FROM inventory_per_sku
GROUP BY productSKU


#Array aggregate
First you need to only select distinct SKUs from the website before joining on other datasets.

We know that there can be more than one product name (like 7" Dog Frisbee) that can share a single SKU.

Let's gather all the possible names into an array

SELECT
  productSKU,
  ARRAY_AGG(DISTINCT v2ProductName) AS push_all_names_into_array
FROM `data-to-insights.ecommerce.all_sessions_raw`
WHERE productSKU = 'GGOEGAAX0098'
GROUP BY productSKU

If you wanted to deduplicate the product names, you could even LIMIT the array like so:

SELECT
  productSKU,
  ARRAY_AGG(DISTINCT v2ProductName LIMIT 1) AS push_all_names_into_array
FROM `data-to-insights.ecommerce.all_sessions_raw`
WHERE productSKU = 'GGOEGAAX0098'
GROUP BY productSKU


Check for items missing in either table

#standardSQL
SELECT DISTINCT
website.productSKU AS website_SKU,
inventory.SKU AS inventory_SKU
FROM `data-to-insights.ecommerce.all_sessions_raw` AS website
FULL JOIN `data-to-insights.ecommerce.products` AS inventory
ON website.productSKU = inventory.SKU
WHERE website.productSKU IS NULL OR inventory.SKU IS NULL


#Array data type

#standardSQL
SELECT
['raspberry', 'blackberry', 'strawberry', 'cherry'] AS fruit_array

#######################
Loading semi-structured JSON into BigQuery
What if you had a JSON file that you needed to ingest into BigQuery?

Create a new table fruit_details in the dataset.

Click on fruit_store dataset, click on the vertical 3-dots, select Open. Now you will see the Create Table option. name the table fruit_details.

Note: You may have to widen your browser window to see the Create table option.
Add the following details for the table:

Source: Choose Google Cloud Storage in the Create table from dropdown.
Select file from Cloud Storage bucket: cloud-training/gsp416/shopping_cart.json
File format: JSONL (Newline delimited JSON)
Call the new table fruit_details.

Check the checkbox of Schema (Auto detect).

Click Create table.

#######################

double array aggregate

SELECT
  fullVisitorId,
  date,
  ARRAY_AGG(v2ProductName) AS products_viewed,
  ARRAY_AGG(pageTitle) AS pages_viewed
  FROM `data-to-insights.ecommerce.all_sessions`
WHERE visitId = 1501570398
GROUP BY fullVisitorId, date
ORDER BY date

## array ARRAY_LENGTH
SELECT
  fullVisitorId,
  date,
  ARRAY_AGG(v2ProductName) AS products_viewed,
  ARRAY_LENGTH(ARRAY_AGG(v2ProductName)) AS num_products_viewed,
  ARRAY_AGG(pageTitle) AS pages_viewed,
  ARRAY_LENGTH(ARRAY_AGG(pageTitle)) AS num_pages_viewed
  FROM `data-to-insights.ecommerce.all_sessions`
WHERE visitId = 1501570398
GROUP BY fullVisitorId, date
ORDER BY date

#array_agg with distinct
SELECT
  fullVisitorId,
  date,
  ARRAY_AGG(DISTINCT v2ProductName) AS products_viewed,
  ARRAY_LENGTH(ARRAY_AGG(DISTINCT v2ProductName)) AS distinct_products_viewed,
  ARRAY_AGG(DISTINCT pageTitle) AS pages_viewed,
  ARRAY_LENGTH(ARRAY_AGG(DISTINCT pageTitle)) AS distinct_pages_viewed
  FROM `data-to-insights.ecommerce.all_sessions`
WHERE visitId = 1501570398
GROUP BY fullVisitorId, date
ORDER BY date

#####
You can do some pretty useful things with arrays like:

finding the number of elements with ARRAY_LENGTH(<array>)

deduplicating elements with ARRAY_AGG(DISTINCT <field>)

ordering elements with ARRAY_AGG(<field> ORDER BY <field>)

limiting ARRAY_AGG(<field> LIMIT 5)
######

The amount of fields available in the Google Analytics schema can be overwhelming for analysis.

######

You will get an error: Cannot access field page on a value with type ARRAY<STRUCT<hitNumber INT64, time INT64, hour INT64, ...>> at [3:8]

Before you can query REPEATED fields (arrays) normally, you must first break the arrays back into rows.

For example, the array for hits.page.pageTitle is stored currently as a single row like:

['homepage','product page','checkout']
and it needs to be

['homepage',
'product page',
'checkout']

How do you do that with SQL?

Answer: Use the UNNEST() function on your array field:

SELECT DISTINCT
  visitId,
  h.page.pageTitle
FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`,
UNNEST(hits) AS h
WHERE visitId = 1501570398
LIMIT 10
Copied!
We'll cover UNNEST() more in detail later but for now just know that:

You need to UNNEST() arrays to bring the array elements back into rows
UNNEST() always follows the table name in your FROM clause (think of it conceptually like a pre-joined table)


#########################
Introduction to STRUCTs
You may have wondered why the field alias hit.page.pageTitle looks like three fields in one separated by periods. Just as ARRAY values give you the flexibility to go deep into the granularity of your fields, another data type allows you to go wide in your schema by grouping related fields together. That SQL data type is the STRUCT data type.

The easiest way to think about a STRUCT is to consider it conceptually like a separate table that is already pre-joined into your main table.

A STRUCT can have:

one or many fields in it
the same or different data types for each field
it's own alias

#################

Note: The .* syntax tells BigQuery to return all fields for that STRUCT (much like it would if totals.* was a separate table we joined against)

Storing your large reporting tables as STRUCTs (pre-joined "tables") and ARRAYs (deep granularity) allows you to:

gain significant performance advantages by avoiding 32 table JOINs

get granular data from ARRAYs when you need it but not be punished if you don't (BigQuery stores each column individually on disk)

have all the business context in one table as opposed to worrying about JOIN keys and which tables have the data you need


#standardSQL
SELECT STRUCT("Rudisha" as name, [23.4, 26.3, 26.4, 26.1] as splits) AS runner

To recap:

Structs are containers that can have multiple field names and data types nested inside.

Arrays can be one of the field types inside of a Struct (as shown above with the splits field).

############

Practice ingesting JSON data
Create a new dataset titled racing.

Create a new table titled race_results. Click on racing dataset and click Create table.

Note: You may have to widen your browser window to see the Create table option.
Source: select Google Cloud Storage under Create table from dropdown.

Select file from Cloud Storage bucket: data-insights-course/labs/optimizing-for-performance/race_results.json

File format: JSONL (Newline delimited JSON)

In Schema, click on Edit as text slider and add the following:

[
    {
        "name": "race",
        "type": "STRING",
        "mode": "NULLABLE"
    },
    {
        "name": "participants",
        "type": "RECORD",
        "mode": "REPEATED",
        "fields": [
            {
                "name": "name",
                "type": "STRING",
                "mode": "NULLABLE"
            },
            {
                "name": "splits",
                "type": "FLOAT",
                "mode": "REPEATED"
            }
        ]
    }
]
Copied!
Click Create table.


####


In traditional relational SQL, if you had a races table and a participants table what would you do to get information from both tables? You would JOIN them together. Here the participant STRUCT (which is conceptually very similar to a table) is already part of your races table but is not yet correlated correctly with your non-STRUCT field "race".

Can you think of what two word SQL command you would use to correlate the 800M race with each of the racers in the first table?

Answer: CROSS JOIN

Great! Now try running this:

#standardSQL
SELECT race, participants.name
FROM racing.race_results
CROSS JOIN
participants  # this is the STRUCT (it is like a table within a table)
Copied!
Table name "participants" missing dataset while no default dataset is set in the request.

Even though the participants STRUCT is like a table, it is still technically a field in the racing.race_results table.

Table name "participants" missing dataset while no default dataset is set in the request.

Even though the participants STRUCT is like a table, it is still technically a field in the racing.race_results table.

Add the dataset name to the query:

#standardSQL
SELECT race, participants.name
FROM racing.race_results
CROSS JOIN
race_results.participants # full STRUCT name
Copied!
And click Run.

Wow! You've successfully listed all of the racers for each race!

You can simplify the last query by:

Adding an alias for the original table
Replacing the words "CROSS JOIN" with a comma (a comma implicitly cross joins)
This will give you the same query result:

#standardSQL
SELECT race, participants.name
FROM racing.race_results AS r, r.participants
Copied!
If you have more than one race type (800M, 100M, 200M), wouldn't a CROSS JOIN just associate every racer name with every possible race like a cartesian product?

Answer: No. This is a correlated cross join which only unpacks the elements associated with a single row. For a greater discussion, see working with ARRAYs and STRUCTs

Recap of STRUCTs:

A SQL STRUCT is simply a container of other data fields which can be of different data types. The word struct means data structure. Recall the example from earlier:

STRUCT(``"Rudisha" as name, [23.4, 26.3, 26.4, 26.1] as splits``)`` AS runner

STRUCTs are given an alias (like runner above) and can conceptually be thought of as a table inside of your main table.

STRUCTs (and ARRAYs) must be unpacked before you can operate over their elements. Wrap an UNNEST() around the name of the struct itself or the struct field that is an array in order to unpack and flatten it.


------

Answer the below questions using the racing.race_results table you created previously.

Task: Write a query to COUNT how many racers were there in total.

To start, use the below partially written query:

#standardSQL
SELECT COUNT(participants.name) AS racer_count
FROM racing.race_results
Copied!
Hint: Remember you will need to cross join in your struct name as an additional data source after the FROM.


2 Answers
#standardSQL
SELECT r.race, count(participants.name)
FROM racing.race_results AS r, r.participants
group by r.race

#standardSQL
SELECT COUNT(p.name) AS racer_count
FROM racing.race_results AS r, UNNEST(r.participants) AS p

#######################
Write a query that will list the total race time for racers whose names begin with R. Order the results with the fastest total time first. Use the UNNEST() operator and start with the partially written query below.

Complete the query:

#standardSQL
SELECT
  p.name,
  SUM(split_times) as total_race_time
FROM racing.race_results AS r
, r.participants AS p
, p.splits AS split_times
WHERE
GROUP BY
ORDER BY
;
Copied!
Hint:

You will need to unpack both the struct and the array within the struct as data sources after your FROM clause
Be sure to use aliases where appropriate


#standardSQL
SELECT
  p.name,
  SUM(split_times) as total_race_time
FROM racing.race_results AS r
, UNNEST(r.participants) AS p
, UNNEST(p.splits) AS split_times
WHERE p.name LIKE 'R%'
GROUP BY p.name
ORDER BY total_race_time ASC;

##################

#standardSQL
SELECT
  p.name,
  split_time
FROM racing.race_results AS r
, UNNEST(r.participants) AS p
, UNNEST(p.splits) AS split_time
WHERE split_time = 23.2;


Build and Execute MySQL, PostgreSQL, and SQLServer to Data Catalog Connectors

gcloud config set project <YOUR_PROJECT_ID>
Copied!
Next set it as an environment variable:

export PROJECT_ID=$(gcloud config get-value project)

gsutil cp gs://spls/gsp814/cloudsql-sqlserver-tooling.zip .
unzip cloudsql-sqlserver-tooling.zip




#############
Connectors

GSP814
Google Cloud selp-paced labs logo

Overview
Data Catalog is a fully managed and scalable metadata management service that empowers organizations to quickly discover, understand, and manage all their data.

It offers a simple and easy-to-use search interface for data discovery, a flexible and powerful cataloging system for capturing both technical and business metadata, and a strong security and compliance foundation with Cloud Data Loss Prevention (DLP) and Cloud Identity and Access Management (IAM) integrations.

Using Data Catalog
There are two main ways you interact with Data Catalog:

Searching for data assets that you have access to.

Tagging assets with metadata.

What you will learn
In this lab, you will learn how to:

Enable the Data Catalog API so that you can use this service in your Google Cloud project.

Execute SQLServer to Data Catalog connector.

Execute PostgreSQL to Data Catalog connector.

Execute MySQL to Data Catalog connector.

Prerequisites
Very Important: Before starting this lab, log out of your personal or corporate gmail account, or run this lab in Incognito. This prevents sign-in confusion while the lab is running.

Setup and requirements
Before you click the Start Lab button
Read these instructions. Labs are timed and you cannot pause them. The timer, which starts when you click Start Lab, shows how long Google Cloud resources will be made available to you.

This hands-on lab lets you do the lab activities yourself in a real cloud environment, not in a simulation or demo environment. It does so by giving you new, temporary credentials that you use to sign in and access Google Cloud for the duration of the lab.

To complete this lab, you need:

Access to a standard internet browser (Chrome browser recommended).
Note: Use an Incognito or private browser window to run this lab. This prevents any conflicts between your personal account and the Student account, which may cause extra charges incurred to your personal account.
Time to complete the lab---remember, once you start, you cannot pause a lab.
Note: If you already have your own personal Google Cloud account or project, do not use it for this lab to avoid extra charges to your account.
How to start your lab and sign in to the Google Cloud Console
Click the Start Lab button. If you need to pay for the lab, a pop-up opens for you to select your payment method. On the left is a panel populated with the temporary credentials that you must use for this lab.

Open Google Console

Copy the username, and then click Open Google Console. The lab spins up resources, and then opens another tab that shows the Sign in page.

Sign in

Tip: Open the tabs in separate windows, side-by-side.

If you see the Choose an account page, click Use Another Account. Choose an account
In the Sign in page, paste the username that you copied from the left panel. Then copy and paste the password.

Important: You must use the credentials from the left panel. Do not use your Google Cloud Training credentials. If you have your own Google Cloud account, do not use it for this lab (avoids incurring charges).

Click through the subsequent pages:

Accept the terms and conditions.
Do not add recovery options or two-factor authentication (because this is a temporary account).
Do not sign up for free trials.
After a few moments, the Cloud Console opens in this tab.

Note: You can view the menu with a list of Google Cloud Products and Services by clicking the Navigation menu at the top-left. Cloud Console Menu
Activate Cloud Shell
Cloud Shell is a virtual machine that is loaded with development tools. It offers a persistent 5GB home directory and runs on the Google Cloud. Cloud Shell provides command-line access to your Google Cloud resources.

In the Cloud Console, in the top right toolbar, click the Activate Cloud Shell button.

Cloud Shell icon

Click Continue.

cloudshell_continue.png

It takes a few moments to provision and connect to the environment. When you are connected, you are already authenticated, and the project is set to your PROJECT_ID. For example:

Cloud Shell Terminal

gcloud is the command-line tool for Google Cloud. It comes pre-installed on Cloud Shell and supports tab-completion.

You can list the active account name with this command:

gcloud auth list
Copied!
(Output)

ACTIVE: *
ACCOUNT: student-01-xxxxxxxxxxxx@qwiklabs.net
To set the active account, run:
    $ gcloud config set account `ACCOUNT`
You can list the project ID with this command:

gcloud config list project
Copied!
(Output)

[core]
project = <project_ID>
(Example output)

[core]
project = qwiklabs-gcp-44776a13dea667a6
For full documentation of gcloud see the gcloud command-line tool overview.
Enable the Data Catalog API
Open the Navigation menu and select APIs and Services > Library.

In the search bar, enter in "Data Catalog" and select the Google Cloud Data Catalog API.

Then click Enable.

If you run into the following error after trying to enable the Data Catalog API:

Failed API Enablement.png

Click Close and refresh your browser tab. Then click Enable again. The Data Catalog API should be successfully enabled:

Successful API Enablement.png

Click Check my progress to verify the objective.
Assessment Completed!
Enable the Data Catalog API
Assessment Completed!

SQLServer to Data Catalog
Start by setting up your environment. Run the following command to set your Project ID, replacing <YOUR_PROJECT_ID> with the Project ID found in the connection details panel:

gcloud config set project <YOUR_PROJECT_ID>
Copied!
Next set it as an environment variable:

export PROJECT_ID=$(gcloud config get-value project)
Copied!
Create the SQLServer Database
In your Cloud Shell session, run the following command to download the scripts to create and populate your SQLServer instance:

gsutil cp gs://spls/gsp814/cloudsql-sqlserver-tooling.zip .
unzip cloudsql-sqlserver-tooling.zip
Copied!
Now change your current working directory to the downloaded directory:

cd cloudsql-sqlserver-tooling
Copied!
Now run the init-db.sh script.

bash init-db.sh
Copied!
This will create your SQLServer instance and populate it with a random schema.

If you get an Error: Failed to load "tfplan" as a plan file, re-run the init-db script.
This will take around 5 to 10 minutes to complete. You can move on when you receive the following output:

CREATE TABLE factory_warehouse15797.employees53b82dc5 ( school80581 REAL, reason91250 DATETIME, randomdata32431 BINARY, phone_number52754 REAL, person66471 REAL, credit_card75527 DATETIME )
COMPLETED
Click Check my progress to verify the objective.
Assessment Completed!
Create the SQLServer Database
Assessment Completed!

Set Up the Service Account
Run the following command to create a Service Account:

gcloud iam service-accounts create sqlserver2dc-credentials \
--display-name  "Service Account for SQLServer to Data Catalog connector" \
--project $PROJECT_ID
Copied!
Now create and download the Service Account Key.

gcloud iam service-accounts keys create "sqlserver2dc-credentials.json" \
--iam-account "sqlserver2dc-credentials@$PROJECT_ID.iam.gserviceaccount.com"
Copied!
Add the Data Catalog admin role to the Service Account:

gcloud projects add-iam-policy-binding $PROJECT_ID \
--member "serviceAccount:sqlserver2dc-credentials@$PROJECT_ID.iam.gserviceaccount.com" \
--quiet \
--project $PROJECT_ID \
--role "roles/datacatalog.admin"
Copied!
Click Check my progress to verify the objective.
Assessment Completed!
Set Up the Service Account for SQLServer
Assessment Completed!

Execute SQLServer to Data Catalog connector.
You can build the SQLServer connector yourself by going to this GitHub repository.

To facilitate its usage, we are going to use a docker image.

The variables needed were output by the Terraform config.

Change directories into the location of the Terraform scripts:

cd infrastructure/terraform/
Copied!
Grab the environment variables:

public_ip_address=$(terraform output -raw public_ip_address)
username=$(terraform output -raw username)
password=$(terraform output -raw password)
database=$(terraform output -raw db_name)
Copied!
Change back to the root directory for the example code:

cd ~/cloudsql-sqlserver-tooling
Copied!
Run the following command to execute the connector:

docker run --rm --tty -v \
"$PWD":/data mesmacosta/sqlserver2datacatalog:stable \
--datacatalog-project-id=$PROJECT_ID \
--datacatalog-location-id=us-central1 \
--sqlserver-host=$public_ip_address \
--sqlserver-user=$username \
--sqlserver-pass=$password \
--sqlserver-database=$database
Copied!
Soon after you should receive the following output:

============End sqlserver-to-datacatalog============
Click Check my progress to verify the objective.
Assessment Completed!
Execute SQLServer to Data Catalog connector
Assessment Completed!

Search for the SQLServer Entries in Data Catalog
After the script finishes, open the navigation menu and select Data Catalog from the list of services.

In the the Data Catalog page, click on Tag Templates.

You should see your sqlserver Tag Templates listed.

Next, select Entry Groups.

You should see the sqlserver Entry Group:

sqlserver-entry-group.png

Now click on the sqlserver Entry Group. Your console should resemble the following:

entry_group.png

This is the real value of an Entry Group—you can see all entries that belong to sqlserver using the UI.

Click on one of the warehouse entries. Look at the Custom entry details and tags:

Custom entry details and Tags:

tag1.png

This is the real value the connector adds — it allows you to have the metadata searchable in Data Catalog.

Cleaning up
To delete the created resources, run the following command to delete the SQLServer metadata:

./cleanup-db.sh
Copied!
Now execute the cleaner container:

docker run --rm --tty -v \
"$PWD":/data mesmacosta/sqlserver-datacatalog-cleaner:stable \
--datacatalog-project-ids=$PROJECT_ID \
--rdbms-type=sqlserver \
--table-container-type=schema
Copied!
Now run the following command to delete the SQLServer database:

./delete-db.sh
Copied!
From the Navigation menu click Data Catalog. Search for sqlserver. You will no longer see the SQLServer Tag Templates in the results:

SQLServerMetadata-removed.png

Ensure you see the following output in Cloud Shell before you move on:

 Cloud SQL Instance deleted
 COMPLETED
You will now learn how to do the same thing with a PostgreSQL instance.

PostgreSQL to Data Catalog
Create the PostgreSQL Database
Run the following command in Cloud Shell to return to your home directory:

cd
Copied!
Run the following command to clone the Github repository:

gsutil cp gs://spls/gsp814/cloudsql-postgresql-tooling.zip .
unzip cloudsql-postgresql-tooling.zip
Copied!
Now change your current working directory to the cloned repo directory:

cd cloudsql-postgresql-tooling
Copied!
Now execute the init-db.sh script:

bash init-db.sh
Copied!
This will create your PostgreSQL instance and populate it with a random schema. This can take around 10 to 15 minutes to complete.

If you get an Error: Failed to load "tfplan" as a plan file, re-run the init-db script.
Soon after you should receive the following output:

CREATE TABLE factory_warehouse69945.home17e97c57 ( house57588 DATE, paragraph64180 SMALLINT, ip_address61569 JSONB, date_time44962 REAL, food19478 JSONB, state8925 VARCHAR(25), cpf75444 REAL, date_time96090 SMALLINT, reason7955 CHAR(5), phone_number96292 INT, size97593 DATE, date_time609 CHAR(5), location70431 DATE )
 COMPLETED
Click Check my progress to verify the objective.
Assessment Completed!
Create the PostgreSQL Database
Assessment Completed!

Set Up the Service Account
Create a Service Account.

gcloud iam service-accounts create postgresql2dc-credentials \
--display-name  "Service Account for PostgreSQL to Data Catalog connector" \
--project $PROJECT_ID
Copied!
Next create and download the Service Account Key.

gcloud iam service-accounts keys create "postgresql2dc-credentials.json" \
--iam-account "postgresql2dc-credentials@$PROJECT_ID.iam.gserviceaccount.com"
Copied!
Next add Data Catalog admin role to the Service Account.

gcloud projects add-iam-policy-binding $PROJECT_ID \
--member "serviceAccount:postgresql2dc-credentials@$PROJECT_ID.iam.gserviceaccount.com" \
--quiet \
--project $PROJECT_ID \
--role "roles/datacatalog.admin"
Copied!
Click Check my progress to verify the objective.
Assessment Completed!
Create a Service Account for postgresql
Assessment Completed!

Execute PostgreSQL to Data Catalog connector
You can build the PostgreSQL connector yourself by going to this GitHub repository.

To facilitate its usage, we are going to use a docker image.

The variables needed were output by the Terraform config.

Change directories into the location of the Terraform scripts:

cd infrastructure/terraform/
Copied!
Grab the environment variables:

public_ip_address=$(terraform output -raw public_ip_address)
username=$(terraform output -raw username)
password=$(terraform output -raw password)
database=$(terraform output -raw db_name)
Copied!
Change back to the root directory for the example code:

cd ~/cloudsql-postgresql-tooling
Copied!
Execute the connector:

docker run --rm --tty -v \
"$PWD":/data mesmacosta/postgresql2datacatalog:stable \
--datacatalog-project-id=$PROJECT_ID \
--datacatalog-location-id=us-central1 \
--postgresql-host=$public_ip_address \
--postgresql-user=$username \
--postgresql-pass=$password \
--postgresql-database=$database
Copied!
Soon after you should receive the following output:

============End postgresql-to-datacatalog============
Click Check my progress to verify the objective.
Assessment Completed!
Execute PostgreSQL to Data Catalog connector
Assessment Completed!

Check the results of the script
Ensure that you are in the Data Catalog home page.

Click on Tag Templates.

You should see the following postgresql Tag Templates:

postgresql-tag-templates.png

Click on Entry groups.

You should see the following postgresql Entry Group:

postgresql-entry-groups.png

Now click on the postgresql Entry Group. Your console should resemble the following:

entry_group_2.png

This is the real value of an Entry Group — you can see all entries that belong to postgresql using the UI.

Click on one of the warehouse entries. Look at the Custom entry details and tags:

Custom entry details and Tags:

tags-2.png

This is the real value the connector adds—it allows you to have the metadata searchable in Data Catalog.

Cleaning up
To delete the created resources, run the following command to delete the PostgreSQL metadata:

./cleanup-db.sh
Copied!
Now execute the cleaner container:

docker run --rm --tty -v \
"$PWD":/data mesmacosta/postgresql-datacatalog-cleaner:stable \
--datacatalog-project-ids=$PROJECT_ID \
--rdbms-type=postgresql \
--table-container-type=schema
Copied!
Finally, delete the PostgreSQL database:

./delete-db.sh
Copied!
Now, from the Navigation menu click on Data Catalog. Search for PostgreSQL. You will no longer see the PostgreSQL Tag Templates in the results:

PostgreSQLServerMetadata-removed.png

Ensure you see the following output in Cloud Shell before you move on:

  Cloud SQL Instance deleted
  COMPLETED
You will now learn how to do the same thing with a MySQL instance.

MySQL to Data Catalog
Create the MySQL Database
Run the following command in Cloud Shell to return to your home directory:

cd
Copied!
Run the following command to download the scripts to create and populate your MySQL instance:

gsutil cp gs://spls/gsp814/cloudsql-mysql-tooling.zip .
unzip cloudsql-mysql-tooling.zip
Copied!
Now change your current working directory to the cloned repo directory:

cd cloudsql-mysql-tooling
Copied!
Next execute the init-db.sh script:

bash init-db.sh
Copied!
This will create your MySQL instance and populate it with a random schema. After a few minutes, you should receive the following output:

CREATE TABLE factory_warehouse14342.persons88a5ebc4 ( address9634 TEXT, cpf12934 FLOAT, food88799 BOOL, food4761 LONGTEXT, credit_card44049 FLOAT, city8417 TINYINT, name76076 DATETIME, address19458 TIME, reason49953 DATETIME )
 COMPLETED
If you get an Error: Failed to load "tfplan" as a plan file, re-run the init-db script.
Click Check my progress to verify the objective.
Assessment Completed!
Create the MySQL Database
Assessment Completed!

Set Up the Service Account
Run the following to create a Service Account:

gcloud iam service-accounts create mysql2dc-credentials \
--display-name  "Service Account for MySQL to Data Catalog connector" \
--project $PROJECT_ID
Copied!
Next, create and download the Service Account Key:

gcloud iam service-accounts keys create "mysql2dc-credentials.json" \
--iam-account "mysql2dc-credentials@$PROJECT_ID.iam.gserviceaccount.com"
Copied!
Next add Data Catalog admin role to the Service Account:

gcloud projects add-iam-policy-binding $PROJECT_ID \
--member "serviceAccount:mysql2dc-credentials@$PROJECT_ID.iam.gserviceaccount.com" \
--quiet \
--project $PROJECT_ID \
--role "roles/datacatalog.admin"
Copied!
Click Check my progress to verify the objective.
Assessment Completed!
Create a Service Account for MySQL
Assessment Completed!

Execute MySQL to Data Catalog connector
You can build the MySQL connector yourself by going to this GitHub repository.

To facilitate its usage, this lab uses a docker image.

The variables needed were output by the Terraform config.

Change directories into the location of the Terraform scripts:

cd infrastructure/terraform/
Copied!
Grab the environment variables:

public_ip_address=$(terraform output -raw public_ip_address)
username=$(terraform output -raw username)
password=$(terraform output -raw password)
database=$(terraform output -raw db_name)
Copied!
Change back to the root directory for the example code:

cd ~/cloudsql-mysql-tooling
Copied!
Execute the connector:

docker run --rm --tty -v \
"$PWD":/data mesmacosta/mysql2datacatalog:stable \
--datacatalog-project-id=$PROJECT_ID \
--datacatalog-location-id=us-central1 \
--mysql-host=$public_ip_address \
--mysql-user=$username \
--mysql-pass=$password \
--mysql-database=$database
Copied!
Soon after you should receive the following output:

============End mysql-to-datacatalog============
Click Check my progress to verify the objective.
Assessment Completed!
Execute MySQL to Data Catalog connector
Assessment Completed!

Check the results of the script
Ensure that you are in the Data Catalog home page.

Click on Tag Templates.

You should see the following mysql Tag Templates:

mysql-tag-templates.png

Click on Entry groups.

You should see the following mysql Entry Group:

mysql-entry-groups.png

Now click on the mysql Entry Group. Your console should resemble the following:

entry-group-3.png

This is the real value of an Entry Group — you can see all entries that belong to MySQL using the UI.

Click on one of the warehouse entries. Look at the Custom entry details and tags:

Custom entry details and Tags:

tags-3.png

This is the real value the connector adds — it allows you to have the metadata searchable in Data Catalog.

Cleaning up
To delete the created resources, run the following command to delete the MySQL metadata:

./cleanup-db.sh
Copied!
Now execute the cleaner container:

docker run --rm --tty -v \
"$PWD":/data mesmacosta/mysql-datacatalog-cleaner:stable \
--datacatalog-project-ids=$PROJECT_ID \
--rdbms-type=mysql \
--table-container-type=database
Copied!
Finally, delete the PostgreSQL database:

./delete-db.sh
Copied!
From the Navigation menu click Data Catalog. Search for MySQL. You will no longer see the MySQL Tag Templates in the results:

MySQLServerServerMetadata-removed.png

Ensure you see the following output in Cloud Shell before you move on:

  Cloud SQL Instance deleted
  COMPLETED
Congratulations!
Great job! You received hands-on practice with Data Catalog connectors.

In this lab, you learned how to:

Enable the Data Catalog API.
Create a dataset.
Copy a public New York Taxi table to your dataset.
Create a tag template and attach the tag to your table.
BigQuery for Data Warehousing Quest BigQuery for Marketing Analysts Quest badge Data Catalog Quest Badge

Finish Your Quest
This self-paced lab is part of the BigQuery for Data Warehousing, BigQuery for Marketing Analysts, and Data Catalog Fundamentals Quests. A Quest is a series of related labs that form a learning path. Completing a Quest earns you a badge to recognize your achievement. You can make your badge (or badges) public and link to them in your online resume or social media account. Enroll in a Quest and get immediate completion credit if you've taken this lab. See other available Quests.

Next Steps / Learn More
Read the Data Catalog Overview
Learn How to search with Data Catalog
Browse the Overview of APIs and Client Libraries
End your lab
When you have completed your lab, click End Lab. Qwiklabs removes the resources you’ve used and cleans the account for you.

You will be given an opportunity to rate the lab experience. Select the applicable number of stars, type a comment, and then click Submit.

The number of stars indicates the following:

1 star = Very dissatisfied
2 stars = Dissatisfied
3 stars = Neutral
4 stars = Satisfied
5 stars = Very satisfied
You can close the dialog box if you don't want to provide feedback.

For feedback, suggestions, or corrections, please use the Support tab.

Google Cloud Training & Certification
...helps you make the most of Google Cloud technologies. Our classes include technical skills and best practices to help you get up to speed quickly and continue your learning journey. We offer fundamental to advanced level training, with on-demand, live, and virtual options to suit your busy schedule. Certifications help you validate and prove your skill and expertise in Google Cloud technologies.

Manual Last Updated March 31, 2022
Lab Last Tested February 22, 2022
Copyright 2022 Google LLC All rights reserved. Google and the Google logo are trademarks of Google LLC. All other company and product names may be trademarks of the respective companies with which they are associated.



https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=covid19_ecdc&page=dataset


bq show --format=prettyjson qwiklabs-gcp-04-3d57a9fd2587:covid_294.oxford_policy_tracker_744 | jq '.schema.fields' 

[
 {
    "mode": "NULLABLE",
    "name": "population",
    "type": "INTEGER"
  },
  {
    "mode": "NULLABLE",
    "name": "country_area",
    "type": "FLOAT"
  },
  {
    "fields": [
      {
        "mode": "NULLABLE",
        "name": "avg_retail",
        "type": "FLOAT"
      },
      {
        "mode": "NULLABLE",
        "name": "avg_grocery",
        "type": "FLOAT"
      },
      {
        "mode": "NULLABLE",
        "name": "avg_parks",
        "type": "FLOAT"
      },
      {
        "mode": "NULLABLE",
        "name": "avg_transit",
        "type": "FLOAT"
      },
      {
        "mode": "NULLABLE",
        "name": "avg_workplace",
        "type": "FLOAT"
      },
      {
        "mode": "NULLABLE",
        "name": "avg_residential",
        "type": "FLOAT"
      }
    ],
    "mode": "REPEATED",
    "name": "mobility",
    "type": "RECORD"
  }
]


https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=covid19_ecdc&page=dataset


https://www.google.com/covid19/mobility/




curl -LO https://opendata.ecdc.europa.eu/covid19/nationalcasedeath_eueea_daily_ei/csv/data.csv

bq load covid_294.popdata data.csv dateRep:string,day,month:integer,year:integer,cases:integer,deaths:integer,countries:string,geoID:string,countryterritoryCode:string,popData2020:integer,continentExp:string

bq ls covid_294

bq show covid_294.popdata

bq query "SELECT name,count FROM babynames.names2010 WHERE gender = 'F' ORDER BY count DESC LIMIT 5"

bq query "SELECT name,count FROM babynames.names2010 WHERE gender = 'M' ORDER BY count ASC LIMIT 5"
